{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# âœ” Introduction\n\nWaste has been detrimental to the environment's health and its improper disposal has caused serious problems to our health as well as our surroundings.\n\nRecycling is the process of converting waste materials into new materials and objects.It can also reduce the waste of potentially useful materials and the consumption of fresh raw materials, lowering energy consumption, air pollution (from incineration), and water pollution (from landfilling). Many types of glass, paper, cardboard, metal, plastic, tyres, textiles, batteries, and electronics are recyclable. Composting and other biodegradable waste reuse, such as food and garden waste, is also a type of recycling. Recycling materials are either delivered to a household recycling centre or collected from curbside bins, where they are sorted, cleaned, and reprocessed into new materials for the manufacture of new products.\nHence separating recyclable and organic waste is the first step towards systematic waste disposal.\n\n **In this notebook, we will use a Convolutional Neural Network(CNN) with Transfer Learning to classify waste as organic or recyclable.**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport cv2\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, _ in os.walk('/kaggle/input'):\n        print(dirname)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-30T18:10:33.917838Z","iopub.execute_input":"2022-05-30T18:10:33.918121Z","iopub.status.idle":"2022-05-30T18:11:15.087614Z","shell.execute_reply.started":"2022-05-30T18:10:33.918057Z","shell.execute_reply":"2022-05-30T18:11:15.086579Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_path = \"/kaggle/input/waste-classification-data/DATASET/TRAIN/\"\ntest_path = \"/kaggle/input/waste-classification-data/DATASET/TEST/\"","metadata":{"execution":{"iopub.status.busy":"2022-05-30T18:11:15.089003Z","iopub.execute_input":"2022-05-30T18:11:15.089358Z","iopub.status.idle":"2022-05-30T18:11:15.094940Z","shell.execute_reply.started":"2022-05-30T18:11:15.089325Z","shell.execute_reply":"2022-05-30T18:11:15.093885Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n!pip install --upgrade tensorflow-model-optimization\nimport tensorflow_model_optimization as tfmot","metadata":{"execution":{"iopub.status.busy":"2022-05-30T18:11:15.097069Z","iopub.execute_input":"2022-05-30T18:11:15.097615Z","iopub.status.idle":"2022-05-30T18:11:29.107049Z","shell.execute_reply.started":"2022-05-30T18:11:15.097575Z","shell.execute_reply":"2022-05-30T18:11:29.106030Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import models, layers\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T19:05:55.221481Z","iopub.execute_input":"2022-05-30T19:05:55.221849Z","iopub.status.idle":"2022-05-30T19:05:55.226220Z","shell.execute_reply.started":"2022-05-30T19:05:55.221810Z","shell.execute_reply":"2022-05-30T19:05:55.225315Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nfrom tensorflow.keras.utils import plot_model\nfrom glob import glob\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.applications.xception import Xception\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T18:12:08.201384Z","iopub.execute_input":"2022-05-30T18:12:08.201859Z","iopub.status.idle":"2022-05-30T18:12:08.208295Z","shell.execute_reply.started":"2022-05-30T18:12:08.201776Z","shell.execute_reply":"2022-05-30T18:12:08.207322Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Visualization","metadata":{}},{"cell_type":"code","source":"x_data = [] \ny_data = [] \n\nfor category in glob(train_path+'/*'):\n    for file in tqdm(glob(category+'/*')):\n        img_array=cv2.imread(file)\n        img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n        x_data.append(img_array) \n        y_data.append(category.split(\"/\")[-1])\n        \ndata=pd.DataFrame({'image': x_data,'label': y_data})","metadata":{"execution":{"iopub.status.busy":"2022-05-30T18:12:21.402335Z","iopub.execute_input":"2022-05-30T18:12:21.402665Z","iopub.status.idle":"2022-05-30T18:15:13.636301Z","shell.execute_reply.started":"2022-05-30T18:12:21.402634Z","shell.execute_reply":"2022-05-30T18:15:13.635463Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-30T18:16:17.884887Z","iopub.execute_input":"2022-05-30T18:16:17.885221Z","iopub.status.idle":"2022-05-30T18:16:17.891903Z","shell.execute_reply.started":"2022-05-30T18:16:17.885191Z","shell.execute_reply":"2022-05-30T18:16:17.891019Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nCounter(y_data)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T18:16:19.437200Z","iopub.execute_input":"2022-05-30T18:16:19.437941Z","iopub.status.idle":"2022-05-30T18:16:19.454778Z","shell.execute_reply.started":"2022-05-30T18:16:19.437887Z","shell.execute_reply":"2022-05-30T18:16:19.450992Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"colors = ['#FEC8D8','#D9F4E0']\nplt.pie(data.label.value_counts(),startangle=90,explode=[0.05,0.05],autopct='%0.2f%%',\n        labels=['Organic', 'Recyclable'], colors= colors,radius=2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T18:16:20.666608Z","iopub.execute_input":"2022-05-30T18:16:20.666953Z","iopub.status.idle":"2022-05-30T18:16:20.796565Z","shell.execute_reply.started":"2022-05-30T18:16:20.666919Z","shell.execute_reply":"2022-05-30T18:16:20.795644Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,15))\nfor i in range(9):\n    plt.subplot(4,3,(i%12)+1)\n    index=np.random.randint(15000)\n    plt.title('This image is of {0}'.format(data.label[index]),fontdict={'size':20,'weight':'bold'})\n    plt.imshow(data.image[index])\n    plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T18:16:24.658062Z","iopub.execute_input":"2022-05-30T18:16:24.658387Z","iopub.status.idle":"2022-05-30T18:16:26.557271Z","shell.execute_reply.started":"2022-05-30T18:16:24.658356Z","shell.execute_reply":"2022-05-30T18:16:26.555521Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           background-color:#FAF4ED;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:black;\">\nO -> Organic\nR -> Recyclable\n</p>\n</div>","metadata":{}},{"cell_type":"code","source":"className = glob(train_path + '/*' )\nnumberOfClass = len(className)\nprint(\"Number Of Class: \",numberOfClass)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T18:16:26.584448Z","iopub.execute_input":"2022-05-30T18:16:26.584708Z","iopub.status.idle":"2022-05-30T18:16:26.594539Z","shell.execute_reply.started":"2022-05-30T18:16:26.584683Z","shell.execute_reply":"2022-05-30T18:16:26.593580Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Convolutional Neural Network - CNN","metadata":{}},{"cell_type":"code","source":"input_size = 244\ndatagenerator = ImageDataGenerator(\n    rescale=1.0 / 255, horizontal_flip=True, vertical_flip=True, zoom_range=0.2, shear_range=0.2, rotation_range = 10,validation_split=0.2\n)\n# Train data\n\ntrain_generator = datagenerator.flow_from_directory(\n    directory=train_path, target_size=(input_size, input_size), class_mode=\"binary\", batch_size=128,subset=\"training\"\n)\n\nvalid_generator = datagenerator.flow_from_directory(\n    directory=train_path, target_size=(input_size, input_size), class_mode=\"binary\", batch_size=128,subset=\"validation\"\n)\n\n# Test data\ntest_generator = datagenerator.flow_from_directory(\n    directory=test_path, target_size=(input_size, input_size), class_mode=\"binary\", batch_size=128\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T18:16:31.680241Z","iopub.execute_input":"2022-05-30T18:16:31.680565Z","iopub.status.idle":"2022-05-30T18:16:36.220841Z","shell.execute_reply.started":"2022-05-30T18:16:31.680535Z","shell.execute_reply":"2022-05-30T18:16:36.219879Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(input_size, input_size, 3))\n\nfor layer in base_model.layers:\n    layer.trainable = False\n\nmodel=Sequential()\nmodel.add(base_model)\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(BatchNormalization())\nmodel.add(Dense(1024,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1024,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1,activation='sigmoid'))\n# Compiling the model\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-30T18:18:55.431372Z","iopub.execute_input":"2022-05-30T18:18:55.431713Z","iopub.status.idle":"2022-05-30T18:18:56.912151Z","shell.execute_reply.started":"2022-05-30T18:18:55.431679Z","shell.execute_reply":"2022-05-30T18:18:56.911331Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Summary\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T18:19:03.508493Z","iopub.execute_input":"2022-05-30T18:19:03.508888Z","iopub.status.idle":"2022-05-30T18:19:03.531635Z","shell.execute_reply.started":"2022-05-30T18:19:03.508854Z","shell.execute_reply":"2022-05-30T18:19:03.530713Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"plot_model(model)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T15:41:46.390894Z","iopub.execute_input":"2022-05-27T15:41:46.391232Z","iopub.status.idle":"2022-05-27T15:41:46.882149Z","shell.execute_reply.started":"2022-05-27T15:41:46.3912Z","shell.execute_reply":"2022-05-27T15:41:46.881191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping_val_loss_callback = EarlyStopping(monitor='val_loss', patience=5)\nearly_stopping_loss_callback = EarlyStopping(monitor='loss', patience=5)\n\ncheckpoint_callback = ModelCheckpoint('model_MobileNetV2.h5', monitor='accuracy', verbose=1, save_best_only=True, mode='auto')\n\nmodel_history = model.fit(\n    train_generator,\n    epochs=15,\n    validation_data=valid_generator,\n    callbacks=[early_stopping_val_loss_callback, early_stopping_loss_callback, checkpoint_callback]\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T18:19:08.548099Z","iopub.execute_input":"2022-05-30T18:19:08.548443Z","iopub.status.idle":"2022-05-30T19:00:35.234884Z","shell.execute_reply.started":"2022-05-30T18:19:08.548413Z","shell.execute_reply":"2022-05-30T19:00:35.233929Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=[10,6])\nplt.plot(model_history.history[\"accuracy\"], label = \"Train acc\")\nplt.plot(model_history.history[\"val_accuracy\"], label = \"Validation acc\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T19:02:01.648711Z","iopub.execute_input":"2022-05-30T19:02:01.649076Z","iopub.status.idle":"2022-05-30T19:02:01.832213Z","shell.execute_reply.started":"2022-05-30T19:02:01.649041Z","shell.execute_reply":"2022-05-30T19:02:01.831219Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(model_history.history['loss'], label = \"Train loss\")\nplt.plot(model_history.history['val_loss'], label = \"Validation loss\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T19:02:06.526515Z","iopub.execute_input":"2022-05-30T19:02:06.526884Z","iopub.status.idle":"2022-05-30T19:02:06.702780Z","shell.execute_reply.started":"2022-05-30T19:02:06.526850Z","shell.execute_reply":"2022-05-30T19:02:06.701696Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model_loss, model_acc = model.evaluate(test_generator)\nprint(\"Model has a loss of %.2f and accuracy %.2f%%\" % (model_loss, model_acc*100))","metadata":{"execution":{"iopub.status.busy":"2022-05-30T19:02:18.663380Z","iopub.execute_input":"2022-05-30T19:02:18.663709Z","iopub.status.idle":"2022-05-30T19:03:11.295317Z","shell.execute_reply.started":"2022-05-30T19:02:18.663677Z","shell.execute_reply":"2022-05-30T19:03:11.294496Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Post Training Quantization","metadata":{}},{"cell_type":"code","source":"def apply_quantization(layer):\n    if (\n        isinstance(layer, layers.Dense)\n        or isinstance(layer, layers.MaxPool2D)\n        or isinstance(layer, layers.Conv2D)\n    ):\n        return tfmot.quantization.keras.quantize_annotate_layer(layer)\n    return layer","metadata":{"execution":{"iopub.status.busy":"2022-05-30T19:03:17.504007Z","iopub.execute_input":"2022-05-30T19:03:17.504346Z","iopub.status.idle":"2022-05-30T19:03:17.509833Z","shell.execute_reply.started":"2022-05-30T19:03:17.504314Z","shell.execute_reply":"2022-05-30T19:03:17.508522Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"annotated_model = tf.keras.models.clone_model(\n    model,\n    clone_function=apply_quantization\n)\n\nquant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\nquant_aware_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T19:06:01.536246Z","iopub.execute_input":"2022-05-30T19:06:01.536562Z","iopub.status.idle":"2022-05-30T19:06:09.202930Z","shell.execute_reply.started":"2022-05-30T19:06:01.536533Z","shell.execute_reply":"2022-05-30T19:06:09.202147Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"quant_aware_model.compile(\n    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T19:09:25.158730Z","iopub.execute_input":"2022-05-30T19:09:25.159121Z","iopub.status.idle":"2022-05-30T19:09:25.177185Z","shell.execute_reply.started":"2022-05-30T19:09:25.159075Z","shell.execute_reply":"2022-05-30T19:09:25.176291Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"q_history = quant_aware_model.fit(train_generator,\n    epochs=15,\n    validation_data=valid_generator\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quant_model_loss, quant_model_acc = quant_aware_model.evaluate(test_generator)\nprint(\"Quantized Model has a loss of %.2f and accuracy %.2f%%\" % (quant_model_loss, quant_model_acc*100))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Converting Quanitization Aware Model to TF Lite Model","metadata":{}},{"cell_type":"code","source":"converter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\nquantized_tflite_model = converter.convert()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T19:49:12.651194Z","iopub.execute_input":"2022-05-30T19:49:12.651525Z","iopub.status.idle":"2022-05-30T19:50:00.694786Z","shell.execute_reply.started":"2022-05-30T19:49:12.651494Z","shell.execute_reply":"2022-05-30T19:50:00.693891Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"type(test_generator)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:06:00.313099Z","iopub.execute_input":"2022-05-30T20:06:00.313537Z","iopub.status.idle":"2022-05-30T20:06:00.321255Z","shell.execute_reply.started":"2022-05-30T20:06:00.313499Z","shell.execute_reply":"2022-05-30T20:06:00.320348Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"dataset = tf.keras.preprocessing.image_dataset_from_directory(\n  test_path,\n  seed=123,\n  image_size=(input_size, input_size),\n  batch_size=128\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:19:47.814567Z","iopub.execute_input":"2022-05-30T20:19:47.814992Z","iopub.status.idle":"2022-05-30T20:19:48.677689Z","shell.execute_reply.started":"2022-05-30T20:19:47.814949Z","shell.execute_reply":"2022-05-30T20:19:48.676715Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"\ndef evaluate_tflite_model(dataset, interpreter):\n    input_index = interpreter.get_input_details()[0][\"index\"]\n    output_index = interpreter.get_output_details()[0][\"index\"]\n\n    prediction_digits = []\n    test_labels = []\n    for image, label in dataset.unbatch().take(dataset.unbatch().cardinality()):\n\n        test_image = np.expand_dims(image, axis=0).astype(np.float32)\n        interpreter.set_tensor(input_index, test_image)\n        interpreter.invoke()\n        \n        output = interpreter.tensor(output_index)\n        digit = np.argmax(output()[0])\n        prediction_digits.append(digit)\n        test_labels.append(label)\n\n    prediction_digits = np.array(prediction_digits)\n    accuracy = (prediction_digits == test_labels).mean()\n    return accuracy\n\ninterpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\ninterpreter.allocate_tensors()\n\ntest_accuracy = evaluate_tflite_model(dataset, interpreter)\n\nprint('Quant TFLite test_accuracy:', test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nmodel_version = 1\n\nwith open(\n    f\"tf-lite-model{model_version}.tflite\",\n    'wb'\n) as f:\n    f.write(quantized_tflite_model)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T21:01:49.990692Z","iopub.execute_input":"2022-05-30T21:01:49.991055Z","iopub.status.idle":"2022-05-30T21:01:50.062172Z","shell.execute_reply.started":"2022-05-30T21:01:49.991023Z","shell.execute_reply":"2022-05-30T21:01:50.061280Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"## Testing with examples","metadata":{}},{"cell_type":"code","source":"input_index = interpreter.get_input_details()[0][\"index\"]\noutput_index = interpreter.get_output_details()[0][\"index\"]\nclass_names = ['organic','recyclable']\nplt.figure(figsize=(15, 15))\nfor images, labels in dataset.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        \n        actual_class = class_names[labels[i]]\n\n        test_image = np.expand_dims(images[i], axis=0).astype(np.float32)\n        interpreter.set_tensor(input_index, test_image)\n        interpreter.invoke()\n        output = interpreter.tensor(output_index)\n        pred = output()[0][0]\n        if pred > 0.5:\n            confidence = np.round(pred*100, decimals=2)\n            predicted_class = 'recyclable'\n        else:\n            confidence = np.round((1-pred)*100, decimals=2)\n            predicted_class = 'organic'\n        plt.title(f\"Actual: {actual_class}\\n Predicted Class: {predicted_class}\\n Confidence: {confidence}%\")\n        plt.axis('off')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T21:21:40.055182Z","iopub.execute_input":"2022-05-30T21:21:40.055572Z","iopub.status.idle":"2022-05-30T21:21:53.842063Z","shell.execute_reply.started":"2022-05-30T21:21:40.055538Z","shell.execute_reply":"2022-05-30T21:21:53.841266Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"file1 = test_path + 'O/O_12568.jpg'\nfile = test_path + 'R/R_10000.jpg'\nimg_array=cv2.imread(file1)\nimg=cv2.imread(file)\n\nimg_array = cv2.resize(img_array, (input_size, input_size))\nimg_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\nimg_array = np.reshape(img_array, [1, input_size, input_size, 3]) / 255.0\n\npred = float(model(img_array).numpy()[0])\nif pred > 0.5:\n    pred = np.round(pred*100, decimals=2)\nelse:\n    pred = np.round((1-pred)*100, decimals=2)\nprint(pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T21:17:04.608702Z","iopub.execute_input":"2022-05-30T21:17:04.609086Z","iopub.status.idle":"2022-05-30T21:17:04.714942Z","shell.execute_reply.started":"2022-05-30T21:17:04.609050Z","shell.execute_reply":"2022-05-30T21:17:04.714074Z"},"trusted":true},"execution_count":60,"outputs":[]}]}